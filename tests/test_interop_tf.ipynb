{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728982580.327655  188118 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728982580.366881  188118 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728982580.369725  188118 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../build/python')\n",
    "\n",
    "import os # Configure which GPU\n",
    "if os.getenv(\"CUDA_VISIBLE_DEVICES\") is None:\n",
    "    gpu_num = 0 # Use \"\" to use the CPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Avoid warnings from TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "import mitsuba as mi\n",
    "import drjit as dr\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append('../drjit')\n",
    "\n",
    "from interop import wrap, to_drjit, from_drjit, flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap('torch', 'drjit')\n",
    "def test_fn(x, y, z):\n",
    "    return x*2, y+1, ~z\n",
    "\n",
    "dt = tf.float32\n",
    "x = tf.constant([0, 1, 2], dtype=dt)\n",
    "y = tf.cast(x, dtype=tf.int32)\n",
    "if True:\n",
    "    z = y > 0\n",
    "else:\n",
    "    z = y\n",
    "\n",
    "a, b, c = test_fn(x, y, z)\n",
    "assert tf.reduce_all(a == x*2)  and tf.reduce_all(b == y+1) and tf.reduce_all(c == ~z)\n",
    "#assert torch.all(a == x*2) and torch.all(b == y + 1) and torch.all(c == ~z)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    a, b, c = test_fn(x, y, z)\n",
    "grad = tape.gradient(a, x, output_gradients=tf.constant([10, 20, 30], dtype=dt))\n",
    "assert tf.reduce_all(grad == tf.constant([20, 40, 60], dtype=dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], <__main__.MyClass at 0x7f81fc191070>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return x\n",
    "tf.nest.map_structure(fun, [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "        pass\n",
    "\n",
    "@wrap('drjit', 'tf')\n",
    "def test_fn(x, y):\n",
    "    return x, y\n",
    "\n",
    "x = dr.arange(dr.cuda.ad.Float32, 3)\n",
    "dr.enable_grad(x)\n",
    "\n",
    "y = MyClass()\n",
    "\n",
    "a, b = test_fn(x, y)\n",
    "assert dr.all(a == x) and b is y\n",
    "\n",
    "a.grad = [10, 20, 30]\n",
    "dr.backward_to(x)\n",
    "assert dr.all(x.grad == [10, 20, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 12, 15]] ([100, 200, 300],)\n"
     ]
    }
   ],
   "source": [
    "@wrap('torch', 'drjit')\n",
    "def test_fn(x):\n",
    "        return {\n",
    "            123:(x[0][\"hello\"] + 2*x[1][\"world\"][0])\n",
    "        }\n",
    "dt = torch.float32\n",
    "x = torch.tensor([1, 2, 3], dtype=dt, requires_grad=True)\n",
    "y = torch.tensor([4, 5, 6], dtype=dt, requires_grad=True)\n",
    "xt = [\n",
    "    { 'hello' : x },\n",
    "    { 'world' : (y,) }\n",
    "]\n",
    "rt = test_fn(xt)\n",
    "r = rt[123]\n",
    "\n",
    "torch.autograd.backward(r, torch.tensor([100, 200, 300], dtype=dt))\n",
    "assert torch.all(x.grad == torch.tensor([100, 200, 300], dtype=dt))\n",
    "assert torch.all(y.grad == torch.tensor([200, 400, 600], dtype=dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 12, 15]\n",
      "[100, 200, 300]\n"
     ]
    }
   ],
   "source": [
    "@wrap('tf', 'drjit')\n",
    "def test_fn(x):\n",
    "        return {\n",
    "            123:(x[0][\"hello\"] + 2*x[1][\"world\"][0])\n",
    "        }\n",
    "\n",
    "dt = tf.float32\n",
    "x = tf.constant([1, 2, 3], dtype=dt)\n",
    "y = tf.constant([4, 5, 6], dtype=dt)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x, y])\n",
    "    xt = [\n",
    "        { 'hello' : x },\n",
    "        { 'world' : (y,) }\n",
    "    ]\n",
    "    rt = test_fn(xt)\n",
    "    r = rt[123]\n",
    "grad = tape.gradient(r, [x, y], output_gradients=tf.constant([100, 200, 300], dtype=dt))\n",
    "assert tf.reduce_all(grad[0] == tf.constant([100, 200, 300], dtype=dt))\n",
    "assert tf.reduce_all(grad[1] == tf.constant([200, 400, 600], dtype=dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "@wrap('tf', 'drjit')\n",
    "def test_fn(x):\n",
    "    return x * 2\n",
    "\n",
    "is_diff = True\n",
    "scalar_deriv = False\n",
    "\n",
    "dt = tf.float32\n",
    "x = tf.cast(tf.range(3), dt)\n",
    "y = test_fn(x)\n",
    "assert tf.reduce_all(y == x * 2)\n",
    "if is_diff:\n",
    "    if scalar_deriv:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            out = test_fn(x)\n",
    "        grad = tape.gradient(out, x)\n",
    "        assert tf.reduce_all(grad == tf.constant([2, 2, 2], dtype=dt))\n",
    "    else:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            out = test_fn(x)\n",
    "        grad = tape.gradient(out, x, output_gradients=tf.constant([10, 20, 30], dtype=dt))\n",
    "        assert tf.reduce_all(grad == tf.constant([20, 40, 60], dtype=dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap('tf', 'drjit')\n",
    "def test_fn(x, y):\n",
    "    return x + y, y, x\n",
    "\n",
    "dt = tf.float32\n",
    "x = tf.constant([0, 1, 2], dtype=dt)\n",
    "y = tf.constant([4], dtype=dt)\n",
    "a, b, c = test_fn(x, y)\n",
    "\n",
    "assert tf.reduce_all(a == tf.constant([4, 5, 6], dtype=dt))\n",
    "assert tf.reduce_all(b == tf.constant([4], dtype=dt))\n",
    "assert tf.reduce_all(c == tf.constant([0, 1, 2], dtype=dt))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x, y])\n",
    "    a, b, c = test_fn(x, y)\n",
    "grad = tape.gradient([a, b, c], [x, y],\n",
    "                    output_gradients=[tf.constant([10, 20, 30], dtype=dt),\n",
    "                                    tf.constant([40], dtype=dt),\n",
    "                                    tf.constant([50, 60, 70], dtype=dt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50., 60., 70.])\n",
      "tensor([40.])\n",
      "tensor([10., 20., 30.])\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.forward_ad as fwd_ad\n",
    "\n",
    "@wrap('torch', 'drjit')\n",
    "def test_fn(x, y):\n",
    "    return x + y, y + 1, x + 1\n",
    "\n",
    "dt = torch.float32\n",
    "x = torch.arange(3, dtype=dt, requires_grad=True)\n",
    "y = torch.tensor([4], dtype=dt, requires_grad=True)\n",
    "xd = torch.tensor([10, 20, 30], dtype=dt)\n",
    "yd = torch.tensor([40], dtype=dt)\n",
    "\n",
    "with fwd_ad.dual_level():\n",
    "    x = fwd_ad.make_dual(x, xd)\n",
    "    y = fwd_ad.make_dual(y, yd)\n",
    "    a, b, c = test_fn(x, y)\n",
    "\n",
    "    a, ad = fwd_ad.unpack_dual(a)\n",
    "    b, bd = fwd_ad.unpack_dual(b)\n",
    "    c, cd = fwd_ad.unpack_dual(c)\n",
    "\n",
    "    print(ad)\n",
    "    print(bd)\n",
    "    print(cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([10. 12. 14.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32) tf.Tensor([3.], shape=(1,), dtype=float32) tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n",
      "Result 1: [4. 5. 6.]\n",
      "Result 2: [5.]\n",
      "Result 3: [1. 2. 3.]\n",
      "Gradient w.r.t x: [2. 2. 2.]\n",
      "Gradient w.r.t y: [4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom operation with multiple outputs and custom gradient\n",
    "@tf.custom_gradient\n",
    "def test_fn(x, y):\n",
    "    # Forward pass: compute the outputs\n",
    "    result1 = x + y\n",
    "    result2 = y + 1\n",
    "    result3 = x + 1\n",
    "\n",
    "    def grad(upstream1, upstream2, upstream3):\n",
    "        print(upstream1, upstream2, upstream3)\n",
    "        # Gradients of the output with respect to inputs x and y\n",
    "        grad_x = upstream1 + upstream3  # Derivative of result1 and result3 w.r.t x\n",
    "        grad_y = upstream1 + upstream2  # Derivative of result1 and result2 w.r.t y\n",
    "        return grad_x, grad_y\n",
    "\n",
    "    return (result1, result2, result3), grad\n",
    "\n",
    "# Example usage of the custom operation\n",
    "x = tf.cast([0,1,2], dtype=dt)\n",
    "y = tf.constant([4], dtype=dt)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x, y])\n",
    "    res1, res2, res3 = test_fn(x, y)\n",
    "    loss = res1 + res2 + res3  # Example loss function using all outputs\n",
    "    print(loss)\n",
    "\n",
    "# Compute gradients with respect to inputs x and y\n",
    "grads = tape.gradient(loss, [x, y])\n",
    "\n",
    "# Print results\n",
    "print('Result 1:', res1.numpy())\n",
    "print('Result 2:', res2.numpy())\n",
    "print('Result 3:', res3.numpy())\n",
    "print('Gradient w.r.t x:', grads[0].numpy())\n",
    "print('Gradient w.r.t y:', grads[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 2. 2.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([2. 2. 2.], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__BroadcastTo_device_/job:localhost/replica:0/task:0/device:GPU:0}} Unable to broadcast tensor of shape [3] to tensor of shape [1] [Op:BroadcastTo] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 40\u001b[0m\n\u001b[1;32m     34\u001b[0m yd \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m40\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mdt)          \u001b[38;5;66;03m# Shape (1,)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mautodiff\u001b[38;5;241m.\u001b[39mForwardAccumulator(\n\u001b[1;32m     37\u001b[0m             primals\u001b[38;5;241m=\u001b[39m[x, y],\n\u001b[1;32m     38\u001b[0m             tangents\u001b[38;5;241m=\u001b[39m[xd, yd]\n\u001b[1;32m     39\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m acc:\n\u001b[0;32m---> 40\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtest_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m grad_out \u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mjvp(out)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Print results of forward-mode gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:339\u001b[0m, in \u001b[0;36mBind.__call__\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk):\n\u001b[0;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:293\u001b[0m, in \u001b[0;36mcustom_gradient.<locals>.decorated\u001b[0;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decorated function with custom gradient.\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 293\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_eager_mode_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _graph_mode_decorator(wrapped, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:593\u001b[0m, in \u001b[0;36m_eager_mode_decorator\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_gradient function expected to return \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradients, but returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(flat_grads)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_grads \u001b[38;5;241m+\u001b[39m variable_grads\n\u001b[0;32m--> 593\u001b[0m \u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecorded_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mactual_grad_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m flat_result \u001b[38;5;241m=\u001b[39m composite_tensor_gradient\u001b[38;5;241m.\u001b[39mreplace_flat_tensors_for_gradients(\n\u001b[1;32m    596\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(result), flat_result)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(result, flat_result)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/eager/record.py:84\u001b[0m, in \u001b[0;36mrecord_operation\u001b[0;34m(op_type, output_tensors, input_tensors, backward_function, forward_function)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecord_operation\u001b[39m(op_type, output_tensors, input_tensors, backward_function,\n\u001b[1;32m     82\u001b[0m                      forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Records the operation on all tapes in the stack.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m   \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeSetRecordOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mforward_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:583\u001b[0m, in \u001b[0;36m_eager_mode_decorator.<locals>.actual_grad_fn\u001b[0;34m(*result_grad_components)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust return gradient for each variable from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@custom_gradient grad_fn.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m   input_grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m   variable_grads \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    585\u001b[0m flat_grads \u001b[38;5;241m=\u001b[39m composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m    586\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(input_grads))\n",
      "Cell \u001b[0;32mIn[84], line 23\u001b[0m, in \u001b[0;36mtest_fn.<locals>.grad\u001b[0;34m(dresult1, dresult2, dresult3)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(grad_y)\n\u001b[1;32m     22\u001b[0m grad_x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mbroadcast_to(grad_x, tf\u001b[38;5;241m.\u001b[39mshape(x))\n\u001b[0;32m---> 23\u001b[0m grad_y \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_x, grad_y\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/gen_array_ops.py:883\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[0;34m(input, shape, name)\u001b[0m\n\u001b[1;32m    881\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 883\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m    885\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__BroadcastTo_device_/job:localhost/replica:0/task:0/device:GPU:0}} Unable to broadcast tensor of shape [3] to tensor of shape [1] [Op:BroadcastTo] name: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom operation with multiple outputs and custom gradient\n",
    "@tf.custom_gradient\n",
    "def test_fn(x, y):\n",
    "    # Forward pass: compute the outputs\n",
    "    result1 = x + y\n",
    "    result2 = y + 1\n",
    "    result3 = x + 1\n",
    "\n",
    "    def grad(dresult1, dresult2, dresult3):\n",
    "        # Gradients of the outputs with respect to inputs x and y\n",
    "        # Each dresult is a tensor representing the upstream gradient for each output\n",
    "\n",
    "        # Compute gradients for x and y\n",
    "        grad_x = dresult1 + dresult3  # Derivative of result1 and result3 w.r.t x\n",
    "        grad_y = dresult1 + dresult2  # Derivative of result1 and result2 w.r.t y\n",
    "\n",
    "        # Broadcast gradients to match input shapes\n",
    "        print(grad_x)\n",
    "        print(grad_y)\n",
    "        grad_x = tf.broadcast_to(grad_x, tf.shape(x))\n",
    "        grad_y = tf.broadcast_to(grad_y, tf.shape(y))\n",
    "\n",
    "        return grad_x, grad_y\n",
    "\n",
    "    return (result1, result2, result3), grad\n",
    "\n",
    "# Example usage with ForwardAccumulator for forward-mode autodiff\n",
    "dt = tf.float32\n",
    "x = tf.cast([0, 1, 2], dtype=dt)  # Shape (3,)\n",
    "y = tf.constant([4], dtype=dt)    # Shape (1,)\n",
    "xd = tf.constant([10, 20, 30], dtype=dt)  # Shape (3,)\n",
    "yd = tf.constant([40], dtype=dt)          # Shape (1,)\n",
    "\n",
    "with tf.autodiff.ForwardAccumulator(\n",
    "            primals=[x, y],\n",
    "            tangents=[xd, yd]\n",
    "        ) as acc:\n",
    "    out = test_fn(x, y)\n",
    "\n",
    "grad_out = acc.jvp(out)\n",
    "\n",
    "# Print results of forward-mode gradients\n",
    "print(\"Gradient of first output:\", grad_out[0].numpy())\n",
    "print(\"Gradient of second output:\", grad_out[1].numpy())\n",
    "print(\"Gradient of third output:\", grad_out[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.rank(x) - tf.rank(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal(shape=[10,1,20], dtype=tf.float32)\n",
    "y = tf.random.normal(shape=[20], dtype=tf.float32)\n",
    "z = tf.broadcast_to(y, x.shape)\n",
    "\n",
    "\n",
    "def get_broadcast_info(original_shape, target_shape):\n",
    "    padded_shape = [1] * (len(target_shape) - len(original_shape)) + list(original_shape)\n",
    "    added_dims = [i for i, (orig_dim, target_dim) in enumerate(zip(padded_shape, target_shape)) if orig_dim == 1 and target_dim != 1]\n",
    "    return added_dims\n",
    "\n",
    "\n",
    "get_broadcast_info(y.shape, z.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([10. 20. 30. 12.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([  0.  60. 360. 324.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([  0.  60. 360. 324.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def fn(x):\n",
    "    y = tf.stop_gradient(x**3)\n",
    "    @tf.custom_gradient\n",
    "    def grad(dy):\n",
    "        grads = tf.stop_gradient(3*x**2*dy)\n",
    "        def grad(dz):\n",
    "            print(dz)\n",
    "            return dz*grads\n",
    "        return grads, grad\n",
    "    return y, grad\n",
    "\n",
    "def fn2(x):\n",
    "    return x**3\n",
    "\n",
    "x = tf.cast([0, 1, 2, 3], dtype=dt)\n",
    "xd = tf.constant([10, 20, 30, 12], dtype=dt)\n",
    "with tf.autodiff.ForwardAccumulator(\n",
    "            primals=x,\n",
    "            tangents=xd\n",
    "        ) as acc:\n",
    "    out = fn(x)\n",
    "grad_out = acc.jvp(out)\n",
    "print(grad_out)\n",
    "\n",
    "with tf.autodiff.ForwardAccumulator(\n",
    "            primals=x,\n",
    "            tangents=xd\n",
    "        ) as acc:\n",
    "    out = fn2(x)\n",
    "grad_out = acc.jvp(out)\n",
    "print(grad_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "@tf.custom_gradient\n",
    "def test_fn(x):\n",
    "    y = tf.stop_gradient(2*x)\n",
    "    def grad(dy):\n",
    "        return 2*d\n",
    "    return y, grad\n",
    "    return y, grad\n",
    "\n",
    "def test_fn2(x):\n",
    "    return 2*x\n",
    "\n",
    "dt = tf.float32\n",
    "x = tf.cast([0, 1, 2], dtype=dt)\n",
    "xd = tf.constant([10, 20, 30], dtype=dt)\n",
    "with tf.autodiff.ForwardAccumulator(\n",
    "            primals=x,\n",
    "            tangents=xd\n",
    "        ) as acc:\n",
    "    out = test_fn(x)\n",
    "grad_out = acc.jvp(out)\n",
    "print(grad_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "\n",
    "for i in zip(a,b):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.], dtype=float32)>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "custom_gradient function expected to return 3 gradients, but returned 2 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[431], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m yd \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m40\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mdt)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mautodiff\u001b[38;5;241m.\u001b[39mForwardAccumulator(\n\u001b[1;32m     36\u001b[0m             primals\u001b[38;5;241m=\u001b[39m[x, y],\n\u001b[1;32m     37\u001b[0m             tangents\u001b[38;5;241m=\u001b[39m[xd, yd]\n\u001b[1;32m     38\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m acc:\n\u001b[0;32m---> 39\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m grad_out \u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mjvp(out)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(grad_out[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:339\u001b[0m, in \u001b[0;36mBind.__call__\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk):\n\u001b[0;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:293\u001b[0m, in \u001b[0;36mcustom_gradient.<locals>.decorated\u001b[0;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decorated function with custom gradient.\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 293\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_eager_mode_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _graph_mode_decorator(wrapped, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:593\u001b[0m, in \u001b[0;36m_eager_mode_decorator\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_gradient function expected to return \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradients, but returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(flat_grads)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_grads \u001b[38;5;241m+\u001b[39m variable_grads\n\u001b[0;32m--> 593\u001b[0m \u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecorded_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mactual_grad_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m flat_result \u001b[38;5;241m=\u001b[39m composite_tensor_gradient\u001b[38;5;241m.\u001b[39mreplace_flat_tensors_for_gradients(\n\u001b[1;32m    596\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(result), flat_result)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(result, flat_result)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/eager/record.py:84\u001b[0m, in \u001b[0;36mrecord_operation\u001b[0;34m(op_type, output_tensors, input_tensors, backward_function, forward_function)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecord_operation\u001b[39m(op_type, output_tensors, input_tensors, backward_function,\n\u001b[1;32m     82\u001b[0m                      forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Records the operation on all tapes in the stack.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m   \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeSetRecordOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mforward_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:588\u001b[0m, in \u001b[0;36m_eager_mode_decorator.<locals>.actual_grad_fn\u001b[0;34m(*result_grad_components)\u001b[0m\n\u001b[1;32m    585\u001b[0m flat_grads \u001b[38;5;241m=\u001b[39m composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m    586\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(input_grads))\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flat_grads) \u001b[38;5;241m!=\u001b[39m arg_count:\n\u001b[0;32m--> 588\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    589\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_gradient function expected to return \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradients, but returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(flat_grads)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m flat_grads \u001b[38;5;241m+\u001b[39m variable_grads\n",
      "\u001b[0;31mValueError\u001b[0m: custom_gradient function expected to return 3 gradients, but returned 2 instead."
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def test_fn(x, y):\n",
    "    results = x + y, y + 1, x + 1\n",
    "    results = [tf.stop_gradient(el) for el in results]\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def grad(*z):\n",
    "        g = z[0]*tf.ones_like(x)+z[2]*tf.ones_like(x), tf.reduce_sum(tf.ones_like(x)*z[0]) + z[1]\n",
    "        g = [tf.stop_gradient(el) for el in g]\n",
    "\n",
    "        def grad2(*s):\n",
    "            res = \n",
    "            return res\n",
    "        return g, grad2\n",
    "\n",
    "    return results, grad\n",
    "\n",
    "def test_fn2(x, y):\n",
    "    results = x + y, y + 1, x + 1\n",
    "    return results\n",
    "\n",
    "@wrap('tf', 'drjit')\n",
    "def test_fn3(x, y):\n",
    "    results = x + y, y + 1, x + 1\n",
    "    return results\n",
    "\n",
    "for f in [test_fn, test_fn2]:\n",
    "\n",
    "    dt = tf.float32\n",
    "    x = tf.cast([0, 1, 2], dtype=dt)\n",
    "    y = tf.constant([4], dtype=dt)\n",
    "    xd = tf.constant([10, 20, 30], dtype=dt)\n",
    "    yd = tf.constant([40], dtype=dt)\n",
    "    with tf.autodiff.ForwardAccumulator(\n",
    "                primals=[x, y],\n",
    "                tangents=[xd, yd]\n",
    "            ) as acc:\n",
    "        out = f(x, y)\n",
    "    grad_out = acc.jvp(out)\n",
    "    print(grad_out[0])\n",
    "    print(grad_out[1])\n",
    "    print(grad_out[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "4.7683716e-07\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.forward_ad as fa\n",
    "\n",
    "x_primal = torch.randn(10, 10, dtype=torch.float32)\n",
    "x_tangent = torch.randn(10, 10, dtype=torch.float32)\n",
    "\n",
    "y_primal = torch.randn(10, 10, dtype=torch.float32)\n",
    "y_tangent = torch.zeros_like(y_primal)\n",
    "\n",
    "def fn(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "with fa.dual_level():\n",
    "    x_dual = fa.make_dual(x_primal, x_tangent)\n",
    "    y_dual = fa.make_dual(y_primal, y_tangent)\n",
    "    z_dual = fn(x_dual, y_dual)\n",
    "    z_tangent = fa.unpack_dual(z_dual).tangent\n",
    "\n",
    "\n",
    "x_primal_tf = tf.constant(x_primal.numpy())\n",
    "x_tangent_tf = tf.constant(x_tangent.numpy())\n",
    "y_primal_tf = tf.constant(y_primal.numpy())\n",
    "y_tangent_tf = tf.constant(y_tangent.numpy())\n",
    "with tf.autodiff.ForwardAccumulator(primals=[x_primal_tf, y_primal_tf],\n",
    "                                    tangents=[x_tangent_tf, y_tangent_tf]) as acc:\n",
    "    z_primal_tf = fn(x_primal_tf, y_primal_tf)\n",
    "    z_tangent_tf = acc.jvp(z_primal_tf)\n",
    "\n",
    "\n",
    "z_tangent.numpy() == z_tangent_tf.numpy()\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(np.max(np.abs(z_tangent_tf - z_tangent)))\n",
    "print(np.max(np.abs(z_primal_tf - z_dual)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x=x):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m test_fn(x, y)\n\u001b[1;32m     14\u001b[0m grad_out \u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mjvp(out)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_all(\u001b[43mgrad_out\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m70\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_all(grad_out[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m40\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_all(grad_out[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "@wrap('tf', 'drjit')\n",
    "def test_fn(x, y):\n",
    "        return x + y#,  y + 1, x + 1\n",
    "dt = tf.float32\n",
    "x = tf.cast(tf.range(3), dtype=dt)\n",
    "y = tf.constant([4], dtype=dt)\n",
    "xd = tf.constant([10, 20, 30], dtype=dt)\n",
    "yd = tf.constant([40], dtype=dt)\n",
    "with tf.autodiff.ForwardAccumulator(\n",
    "            primals=[x, y],\n",
    "            tangents=[xd, yd]\n",
    "        ) as acc:\n",
    "    out = test_fn(x, y)\n",
    "grad_out = acc.jvp(out)\n",
    "assert tf.reduce_all(grad_out[0] == [50, 60, 70])\n",
    "assert tf.reduce_all(grad_out[1] == [40])\n",
    "assert tf.reduce_all(grad_out[2] == [10, 20, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([2., 2., 2.], dtype=float32)>}\n",
      "tf.Tensor([2. 4. 8.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = {'x' : tf.constant([1, 2, 4], tf.float32)}\n",
    "y = tf.constant([1, 2, 4], tf.float32)\n",
    "def f(x, y):\n",
    "    return 2*x['x'] + y**2\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    tape.watch(y)\n",
    "    z = f(x, y)\n",
    "grad_x = tape.gradient(z, x)\n",
    "grad_y = tape.gradient(z, y)\n",
    "print(grad_x)\n",
    "print(grad_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([{'hello': tensor([100., 200., 300.])}, {'world': (tensor([200., 400., 600.]),)}],)\n"
     ]
    }
   ],
   "source": [
    "@wrap('drjit', 'torch')\n",
    "def test_fn(x):\n",
    "    return {\n",
    "        123:(x[0][\"hello\"] + 2*x[1][\"world\"][0])\n",
    "    }\n",
    "x = dr.llvm.ad.Float(1, 2, 3)\n",
    "y = dr.llvm.ad.Float(4, 5, 6)\n",
    "dr.enable_grad(x, y)\n",
    "xt = [\n",
    "    { 'hello' : x },\n",
    "    { 'world' : (y,) }\n",
    "]\n",
    "rt = test_fn(xt)\n",
    "r = rt[123]\n",
    "\n",
    "r.grad = [100, 200, 300]\n",
    "dr.backward_to(x, y)\n",
    "#print(y.grad)\n",
    "assert dr.all(x.grad == [100, 200, 300])\n",
    "assert dr.all(y.grad == [200, 400, 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([{'hello': <tf.Tensor: shape=(3,), dtype=float32, numpy=array([100., 200., 300.], dtype=float32)>}, {'world': (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([200., 400., 600.], dtype=float32)>,)}],)\n"
     ]
    }
   ],
   "source": [
    "@wrap('drjit', 'tf')\n",
    "def test_fn(x):\n",
    "    return {\n",
    "        123:(x[0][\"hello\"] + 2*x[1][\"world\"][0])\n",
    "    }\n",
    "x = dr.llvm.ad.Float(1, 2, 3)\n",
    "y = dr.llvm.ad.Float(4, 5, 6)\n",
    "dr.enable_grad(x, y)\n",
    "xt = [\n",
    "    { 'hello' : x },\n",
    "    { 'world' : (y,) }\n",
    "]\n",
    "rt = test_fn(xt)\n",
    "r = rt[123]\n",
    "\n",
    "r.grad = [100, 200, 300]\n",
    "dr.backward_to(x, y)\n",
    "#print(y.grad)\n",
    "assert dr.all(x.grad == [100, 200, 300])\n",
    "assert dr.all(y.grad == [200, 400, 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Passed in object 4 of type 'int', not tf.Tensor or tf.Variable or ExtensionType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dr\u001b[38;5;241m.\u001b[39mall(a \u001b[38;5;241m==\u001b[39m x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dr\u001b[38;5;241m.\u001b[39mall(b \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dr\u001b[38;5;241m.\u001b[39mall(c \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     12\u001b[0m a\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m \u001b[43mdr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dr\u001b[38;5;241m.\u001b[39mall(x\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m60\u001b[39m])\n",
      "File \u001b[0;32m~/work/projects/sionna-rt-v-1/mitsuba3/ext/drjit/tests/interop.py:427\u001b[0m, in \u001b[0;36mWrapADOp.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(tensor\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m--> 427\u001b[0m         \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m         tape\u001b[38;5;241m.\u001b[39mwatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    429\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:873\u001b[0m, in \u001b[0;36mGradientTape.watch\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor):\n\u001b[1;32m    865\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that `tensor` is being traced by this tape.\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m    ValueError: if it encounters something that is not a tensor.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_extract_tensors_and_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbackprop_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIsTrainable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_first_n\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWARN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe dtype of the watched tensor must be \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    877\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloating (e.g. tf.float32), got \u001b[39;49m\u001b[38;5;132;43;01m%r\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:700\u001b[0m, in \u001b[0;36m_extract_tensors_and_variables\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[38;5;28;01myield from\u001b[39;00m _extract_tensors_and_variables(components)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed in object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, not tf.Tensor or tf.Variable or ExtensionType.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Passed in object 4 of type 'int', not tf.Tensor or tf.Variable or ExtensionType."
     ]
    }
   ],
   "source": [
    "## Forward-mode AD\n",
    "@wrap('drjit', 'tf')\n",
    "def test_fn(x, y, z):\n",
    "    return x*2, y, z\n",
    "\n",
    "x = dr.arange(dr.llvm.ad.Float, 3)\n",
    "dr.enable_grad(x)\n",
    "\n",
    "a, b, c = test_fn(x, 4, 5.0)\n",
    "assert dr.all(a == x*2) and dr.all(b == 4) and dr.all(c == 5)\n",
    "\n",
    "a.grad = [10, 20, 30]\n",
    "dr.backward_to(x)\n",
    "\n",
    "assert dr.all(x.grad == [20, 40, 60])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 2], dtype=int32)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor([1. 2. 3. 4.], shape=(4,), dtype=float32)\n",
      "y: tf.Tensor([ 1.  4.  9. 16.], shape=(4,), dtype=float32)\n",
      "Gradient: tf.Tensor([2. 4. 6. 8.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test for a single tensor input\n",
    "\n",
    "@wrap('tf', 'drjit')\n",
    "def dr_func(input):\n",
    "    return dr.power(input, 2)\n",
    "\n",
    "x = tf.constant([1,2,3,4], tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = dr_func(x)\n",
    "grad = tape.gradient(y, x)\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"Gradient:\", grad)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: tf.Tensor([ 3. 14. 39. 84.], shape=(4,), dtype=float32)\n",
      "Gradient:\n",
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([2. 4. 6. 8.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([ 3. 12. 27. 48.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test with a list of tensors\n",
    "\n",
    "@wrap('tf', 'drjit')\n",
    "def dr_func(inputs):\n",
    "    result = 0\n",
    "    for i, input in enumerate(inputs):\n",
    "        result += dr.power(input, i+1)\n",
    "    return result\n",
    "\n",
    "x1 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x2 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x3 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "vars = [x1, x2, x3]\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(vars)\n",
    "    result = dr_func(vars)\n",
    "grad = tape.gradient(result, vars)\n",
    "print(\"Result:\", result)\n",
    "print(\"Gradient:\")\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: tf.Tensor([  4.  30. 120. 340.], shape=(4,), dtype=float32)\n",
      "Gradient:\n",
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "[<tf.Tensor: shape=(4,), dtype=float32, numpy=array([2., 4., 6., 8.], dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3., 12., 27., 48.], dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([  4.,  32., 108., 256.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# Test with a nested list of tensors\n",
    "# Test with a list of tensors\n",
    "\n",
    "@wrap('tf', 'drjit')\n",
    "def dr_func(inputs):\n",
    "    inputs = sum(inputs, [])\n",
    "    result = 0\n",
    "    for i, input in enumerate(inputs):\n",
    "        result += dr.power(input, i+1)\n",
    "    return result\n",
    "\n",
    "x1 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x2 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x3 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x4 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "vars = [[x1, x2], [x3, x4]]\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(vars)\n",
    "    result = dr_func(vars)\n",
    "grad = tape.gradient(result, [x1, [x2, x3, x4]])\n",
    "print(\"Result:\", result)\n",
    "print(\"Gradient:\")\n",
    "for g in grad:\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: tf.Tensor([ 2. 10. 30. 68.], shape=(4,), dtype=float32)\n",
      "Gradient:\n",
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([ 3. 12. 27. 48.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple arguments\n",
    "@wrap('tf', 'drjit')\n",
    "def dr_func(x1, x2):\n",
    "    return dr.power(x1, 1) + dr.power(x2, 3)\n",
    "\n",
    "x1 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x2 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x1, x2])\n",
    "    result = dr_func(x1, x2)\n",
    "grad = tape.gradient(result, [x1, x2])\n",
    "print(\"Result:\", result)\n",
    "print(\"Gradient:\")\n",
    "for g in grad:\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 4.,  6.,  8., 10.], dtype=float32)>, None]\n"
     ]
    }
   ],
   "source": [
    "# Test with keyword argument\n",
    "@wrap('tf', 'drjit')\n",
    "def dr_func(*args, **kwargs):\n",
    "    result = 0\n",
    "    for i, x in enumerate(args[0]):\n",
    "        result += dr.power(x, i+1)\n",
    "\n",
    "    for i, x in enumerate(kwargs.values()):\n",
    "        result += 4*x\n",
    "\n",
    "    return result\n",
    "\n",
    "x1 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x2 = tf.constant([2, 3, 4, 5], dtype=tf.float32)\n",
    "x3 = tf.constant([3, 4, 4, 5], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x1, x2, x3])\n",
    "    result = dr_func([x1, x2], x3=x3)\n",
    "grad = tape.gradient(result, [x1, x2, x3])\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([320. 332. 336. 348.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([4. 4. 4. 4.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test with dict argument\n",
    "@wrap('tf', 'drjit')\n",
    "def dr_func(*args, **kwargs):\n",
    "    result = 0\n",
    "    for i, x in enumerate(args[0]):\n",
    "        result += dr.power(x, i+1)\n",
    "\n",
    "    for i, x in enumerate(args[1].values()):\n",
    "        result += 4*(i+1)*x\n",
    "\n",
    "    return result\n",
    "\n",
    "def tf_func(*args, **kwargs):\n",
    "    result = 0\n",
    "    for i, x in enumerate(args[0]):\n",
    "        result += tf.pow(x, i+1)\n",
    "\n",
    "    for i, x in enumerate(args[1].values()):\n",
    "        result += 4*(i+1)*x\n",
    "\n",
    "    return result\n",
    "\n",
    "x1 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x2 = tf.constant([2, 3, 4, 5], dtype=tf.float32)\n",
    "x3 = tf.constant([3, 4, 4, 5], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x1, x2, x3])\n",
    "    result = dr_func(x1, {'x2' : x2, 'x3': x3})\n",
    "grad = tape.gradient(result, [x2])\n",
    "print(result)\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.autodiff import ForwardAccumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "Result: tf.Tensor([2. 4. 6. 8.], shape=(4,), dtype=float32)\n",
      "Gradient:\n",
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test with a non differentiable input\n",
    "@wrap('tf', 'drjit')\n",
    "def dr_func(x1, x2):\n",
    "    return dr.power(x1, 1) + dr.power(x2, 1)\n",
    "\n",
    "def tf_func(x1, x2):\n",
    "    return tf.pow(x1, 1) + tf.cast(tf.pow(x2, 1), x1.dtype)\n",
    "\n",
    "x1 = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n",
    "x2 = tf.constant([1, 2, 3, 4], dtype=tf.int32)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x1, x2])\n",
    "    result = dr_func(x1, x2)\n",
    "grad = tape.gradient(result, [x1, x2])\n",
    "print(\"Result:\", result)\n",
    "print(\"Gradient:\")\n",
    "for g in grad:\n",
    "    print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.constant([1,2,3,4], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import drjit as dr\n",
    "from drjit.cuda.ad import Float\n",
    "from drjit.cuda.ad import Int32\n",
    "\n",
    "t = Int32\n",
    "is_diff = False\n",
    "\n",
    "@wrap('drjit', 'torch')\n",
    "def test_fn(x):\n",
    "    return x * 2\n",
    "\n",
    "x = dr.arange(t, 3)\n",
    "dr.enable_grad(x)\n",
    "y = test_fn(x)\n",
    "assert dr.all(y == [0, 2, 4])\n",
    "\n",
    "\n",
    "if is_diff:\n",
    "    y.grad = [10, 20, 30]\n",
    "    dr.backward_to(x)\n",
    "    assert dr.all(x.grad == [20, 40, 60])\n",
    "else:\n",
    "    assert not dr.grad_enabled(y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interop import from_drjit, to_drjit\n",
    "\n",
    "t = dr.llvm.ad.Float\n",
    "x = dr.arange(t, 3)\n",
    "z, _ = from_drjit(x, 'tf', True)\n",
    "y = z*2\n",
    "z_ = to_drjit(y, 'tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import drjit as dr\n",
    "\n",
    "\n",
    "@wrap('drjit', 'tf')\n",
    "def test_fn(x):\n",
    "    y = x*2\n",
    "    print(y.device)\n",
    "    return y\n",
    "\n",
    "t = dr.cuda.ad.Float\n",
    "x = dr.arange(t, 3)\n",
    "y = test_fn(x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
